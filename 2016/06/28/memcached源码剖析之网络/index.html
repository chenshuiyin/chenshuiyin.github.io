<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="memcached 网络," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="参考博客：http://blog.csdn.net/column/details/memcached-src.html
   代码来自：https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22

此博客完全是自己在学习过程中的笔记，没有任何商业用途
memcached网络模型图
简单叙述就是memcached">
<meta property="og:type" content="article">
<meta property="og:title" content="memcached源码剖析之网络">
<meta property="og:url" content="http://yoursite.com/2016/06/28/memcached源码剖析之网络/index.html">
<meta property="og:site_name" content="Practice makes perfect">
<meta property="og:description" content="参考博客：http://blog.csdn.net/column/details/memcached-src.html
   代码来自：https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22

此博客完全是自己在学习过程中的笔记，没有任何商业用途
memcached网络模型图
简单叙述就是memcached">
<meta property="og:image" content="https://github.com/chenshuiyin/ImageCache/raw/master/memcached.png">
<meta property="og:image" content="https://github.com/chenshuiyin/ImageCache/raw/master/网络模型.png">
<meta property="og:updated_time" content="2016-06-28T02:21:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="memcached源码剖析之网络">
<meta name="twitter:description" content="参考博客：http://blog.csdn.net/column/details/memcached-src.html
   代码来自：https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22

此博客完全是自己在学习过程中的笔记，没有任何商业用途
memcached网络模型图
简单叙述就是memcached">
<meta name="twitter:image" content="https://github.com/chenshuiyin/ImageCache/raw/master/memcached.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2016/06/28/memcached源码剖析之网络/"/>

  <title> memcached源码剖析之网络 | Practice makes perfect </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Practice makes perfect</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                memcached源码剖析之网络
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-06-28T00:03:20+08:00" content="2016-06-28">
              2016-06-28
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>   参考博客：<br><a href="http://blog.csdn.net/column/details/memcached-src.html" target="_blank" rel="external">http://blog.csdn.net/column/details/memcached-src.html</a></p>
<p>   代码来自：<br><a href="https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22" target="_blank" rel="external">https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22</a></p>
</blockquote>
<p>此博客完全是自己在学习过程中的笔记，没有任何商业用途</p>
<h2 id="memcached网络模型图"><a href="#memcached网络模型图" class="headerlink" title="memcached网络模型图"></a>memcached网络模型图</h2><p><img src="https://github.com/chenshuiyin/ImageCache/raw/master/memcached.png" alt="memcached网络"></p>
<p>简单叙述就是memcached网络框架分为主线程（main thread）和工作线程（worker thread），主线程负责连接的建立，工作线程负责请求消息体的接收、处理、响应以及连接的关闭，这种没有脱离《Unix网络编程》里面所列出的几种设计范式（各种模型的使用场景可以参数此书），实际上就是下图中的第8种。</p>
<p><img src="https://github.com/chenshuiyin/ImageCache/raw/master/网络模型.png" alt="memcached网络"></p>
<h2 id="CQ队列及基本操作"><a href="#CQ队列及基本操作" class="headerlink" title="CQ队列及基本操作"></a>CQ队列及基本操作</h2><p>从网络模型图中我们看到每个工作线程都有一个CQ队列，对应的源码如下。</p>
<pre><code>typedef struct conn_queue_item  CQ_ITEM;

struct conn_queue_item {
    int               sfd;
    enum conn_states  init_state;
    int               event_flags;
    int               read_buffer_size;
    enum network_transport     transport;
    CQ_ITEM          *next;
};

/* A connection queue. */
typedef struct conn_queue  CQ;
struct conn_queue {
    CQ_ITEM *head;//指向队列的第一个节点
    CQ_ITEM *tail;//指向队列的最后一个节点
    pthread_mutex_t lock; //一个队列就对应一个锁
};
</code></pre><p>对于每个工作线程来说，这个队列对应的是一个生产者（main thread）和一个消费者（worker thread），但是仍然需要加锁，phread_mutex_t类型的lock就是用来锁住此队列。然后我们来看一下队列的基本操作（push，pop）的源码。</p>
<pre><code>static void cq_init(CQ *cq) {
    pthread_mutex_init(&amp;cq-&gt;lock, NULL);
    cq-&gt;head = NULL;
    cq-&gt;tail = NULL;
}

static CQ_ITEM *cq_pop(CQ *cq) {
    CQ_ITEM *item;

    pthread_mutex_lock(&amp;cq-&gt;lock);
    item = cq-&gt;head;
    if (NULL != item) {
        cq-&gt;head = item-&gt;next;
        if (NULL == cq-&gt;head)
            cq-&gt;tail = NULL;
    }
    pthread_mutex_unlock(&amp;cq-&gt;lock);

    return item;
}

/*
 * Adds an item to a connection queue.
 */
static void cq_push(CQ *cq, CQ_ITEM *item) {
    item-&gt;next = NULL;

    pthread_mutex_lock(&amp;cq-&gt;lock);
    if (NULL == cq-&gt;tail)
          cq-&gt;head = item;
    else
        cq-&gt;tail-&gt;next = item;
    cq-&gt;tail = item;
    pthread_mutex_unlock(&amp;cq-&gt;lock);
}
</code></pre><p>每个worker thread是如何持有这个队列的呢，代码如下。</p>
<pre><code>typedef struct {
    ...
    struct conn_queue *new_conn_queue; /* queue of new connections to handle */
    ...
} LIBEVENT_THREAD;
</code></pre><p>这个LIBEVENT_THREAD顾名思义就是worker线程对应的结构体（为了避免一叶障目，不见森林，我只会在用到时show这部分code，其它部分暂时省略）。</p>
<h2 id="主线程与CQ队列"><a href="#主线程与CQ队列" class="headerlink" title="主线程与CQ队列"></a>主线程与CQ队列</h2><p>当一个连接在主线程建立之后，主线程需要创建一个CQ_ITEM结构体并将它push到对应的工作线程的CQ队列中，这个时候并没有直接去malloc，这样有两个缺点，第一，malloc对应一个系统调用，具有相对昂贵的开销，第二，如果每次需要就malloc容易造成内存碎片，为此，作者使用了内存池，预分配一块比较大的内存，将这块大内存切分成多个CQ_ITEM，代码如下。</p>
<pre><code>//本函数采用了一些优化手段.并非每调用一次本函数就申请一块内存。这会导致
//内存碎片。这里采取的优化方法是，一次性分配64个CQ_ITEM大小的内存(即预分配).
//下次调用本函数的时候，直接从之前分配64个中要一个即可。
//由于是为了防止内存碎片，所以不是以链表的形式放置这64个CQ_ITEM。而是数组的形式。
//于是，cqi_free函数就有点特别了。它并不会真正释放.而是像内存池那样归还
static CQ_ITEM *cqi_new(void) {
    //所有线程都会访问cqi_freelist的。所以需要加锁
    CQ_ITEM *item = NULL;
    pthread_mutex_lock(&amp;cqi_freelist_lock);
    if (cqi_freelist) {
        item = cqi_freelist;
           cqi_freelist = item-&gt;next;
    }
    pthread_mutex_unlock(&amp;cqi_freelist_lock);

    if (NULL == item) {//没有多余的CQ_ITEM了
        int i;

        item = malloc(sizeof(CQ_ITEM) * ITEMS_PER_ALLOC);//该宏等于64

        //item[0]直接返回为调用者，不用next指针连在一起。调用者负责将
        //item[0].next赋值为NULL
        for (i = 2; i &lt; ITEMS_PER_ALLOC; i++)//将这块内存分成一个个的item并且用next指针像链表一样连起来
            item[i - 1].next = &amp;item[i];

            pthread_mutex_lock(&amp;cqi_freelist_lock);
            //因为主线程负责申请CQ_ITEM，子线程负责释放CQ_ITEM。所以cqi_freelist此刻
            //可能并不等于NULL。由于使用头插法，所以无论cqi_freeelist是否为NULL，都能
            //把链表连起来的。
            item[ITEMS_PER_ALLOC - 1].next = cqi_freelist;
            cqi_freelist = &amp;item[1];
            pthread_mutex_unlock(&amp;cqi_freelist_lock);
    }

    return item;
}


//并非释放，而是像内存池那样归还
static void cqi_free(CQ_ITEM *item) {
    pthread_mutex_lock(&amp;cqi_freelist_lock);
    item-&gt;next = cqi_freelist;
    cqi_freelist = item;  //头插法归还
    pthread_mutex_unlock(&amp;cqi_freelist_lock);
}
</code></pre><p>所有工作线程在释放CQ_ITEM的时候会将CQ_ITEM以头插法插入到cqi_freelist，下一次主线程直接取头节点重用，因为存在多个线程去修改它，所以也需要加锁。<br>主线程是如何做到建立连接之后将CQ_ITEM添加到某个工作线程的队列中并通知此线程呢？首先我们来看main函数。</p>
<pre><code>int main (int argc, char **argv) {
    //对memcached的关键设置取默认值
    settings_init();

    //main_base是一个struct event_base类型的全局变量
    main_base = event_init();//为主线程创建一个event_base

    conn_init();//先不管，后面会说到

    //创建settings.num_threads个worker线程，并且为每个worker线程创建一个CQ队列
    //并为这些worker申请各自的event_base，worker线程然后进入事件循环中    
    thread_init(settings.num_threads, main_base);

    //设置一个定时event(也叫超时event)，定时(频率为一秒)更新current_time变量
    //这个超时event是add到全局变量main_base里面的，
    //所以主线程负责更新current_time(这是一个很重要的全局变量)
    clock_handler(0, 0, 0);


    /* create the listening socket, bind it, and init */
    if (settings.socketpath == NULL) {
        FILE *portnumber_file = NULL;
        //创建监听客户端的socket
        //tcp_transport是枚举类型
        if (settings.port &amp;&amp; 
            server_sockets(settings.port, tcp_transport,portnumber_file)) {
            vperror(&quot;failed to listen on TCP port %d&quot;, settings.port);
            exit(EX_OSERR);
        }
    }

    if (event_base_loop(main_base, 0) != 0) {//主线程进入事件循环
        retval = EXIT_FAILURE;
    }

    return retval;
}
</code></pre><p>main函数中创建了属于自己的event_base，thread_init函数是初始化worker thread的，这个我们下面再说，server_sockets负责创建socket并加入可读事件回调，然后主线程就进入event_base_loop事件循环中，下面我们来看看server_sockets函数以及它所调用的其它函数。</p>
<pre><code>//port是默认的11211或者用户使用-p选项设置的端口号
//主线程在main函数会调用本函数
static int server_sockets(int port, 
                          enum network_transport transport,
                          FILE *portnumber_file) {

    //settings.inter里面可能有多个IP地址.如果有多个那么将用逗号分隔
    char *b;
    int ret = 0;
    //复制一个字符串，避免下面的strtok_r函数修改(污染)全局变量settings.inter
    char *list = strdup(settings.inter);

    //这个循环主要是处理多个IP的情况
    for (char *p = strtok_r(list, &quot;;,&quot;, &amp;b);
        p != NULL; //分割出一个个的ip,使用分号;作为分隔符
         p = strtok_r(NULL, &quot;;,&quot;, &amp;b)) {
        int the_port = port;
        char *s = strchr(p, &apos;:&apos;);//启动的可能使用-l ip:port 参数形式
        //ip后面接着端口号，即指定ip的同时也指定了该ip的端口号
        //此时采用ip后面的端口号，而不是采用-p指定的端口号
        if (s != NULL) {
            *s = &apos;\0&apos;;//截断后面的端口号，使得p指向的字符串只是一个ip
            ++s;
            if (!safe_strtol(s, &amp;the_port)) {//非法端口号参数值
                return 1;
            }
        }
        if (strcmp(p, &quot;*&quot;) == 0) {
            p = NULL;
        }
        //处理其中一个IP。有p指定ip(或者hostname)
        ret |= server_socket(p, the_port, transport, portnumber_file);
    }
    free(list);
    return ret;
}


static conn *listen_conn = NULL;//监听队列(可能要同时监听多个IP)

//interface是一个ip、hostname或者NULL。这个ip字符串后面没有端口号。端口号由参数port指出
static int server_socket(const char *interface,
                          int port,
                          enum network_transport transport,
                          FILE *portnumber_file) {
    int sfd;
    struct linger ling = {0, 0};
    struct addrinfo *ai;
    struct addrinfo *next;
    struct addrinfo hints = { .ai_flags = AI_PASSIVE,
                                .ai_family = AF_UNSPEC };
    char port_buf[NI_MAXSERV];
    int success = 0;
    int flags =1;

    hints.ai_socktype = IS_UDP(transport) ? SOCK_DGRAM : SOCK_STREAM;


    snprintf(port_buf, sizeof(port_buf), &quot;%d&quot;, port);
    getaddrinfo(interface, port_buf, &amp;hints, &amp;ai);

    //如果interface是一个hostname的话，那么可能就有多个ip
    for (next= ai; next; next= next-&gt;ai_next) {
        conn *listen_conn_add;

        //创建一个套接字，然后设置为非阻塞的
        sfd = new_socket(next);//调用socket函数
        bind(sfd, next-&gt;ai_addr, next-&gt;ai_addrlen);

        success++;
        listen(sfd, settings.backlog);


        if (!(listen_conn_add = conn_new(sfd, conn_listening,
                                         EV_READ | EV_PERSIST, 1,
                                     t     ransport, main_base))) {
            fprintf(stderr, &quot;failed to create listening connection\n&quot;);
            exit(EXIT_FAILURE);
        }

        //将要监听的多个conn放到一个监听队列里面
        listen_conn_add-&gt;next = listen_conn;
        listen_conn = listen_conn_add;

    }

    freeaddrinfo(ai);

    /* Return zero iff we detected no errors in starting up connections */
    return success == 0;
}


static int new_socket(struct addrinfo *ai) {
    int sfd;
    int flags;
    sfd = socket(ai-&gt;ai_family, ai-&gt;ai_socktype, ai-&gt;ai_protocol);
    flags = fcntl(sfd, F_GETFL, 0);
    fcntl(sfd, F_SETFL, flags | O_NONBLOCK);

    return sfd;
}
</code></pre><p>我们需要关心的是conn_new这个函数，它既会被主线程调用，也会被工作线程调用，下面我们来看看它的源码。</p>
<pre><code>//thread_libevent_process这个管道事件回调使用于主线程接受到客户端连接后通知工作子线程重新创建一个新的
//conn，在conn_new重新设置网络事件回调函数conn_new-&gt;event_handler

//为sfd分配一个conn结构体，并且为这个sfd建立一个event，然后base监听这个event
//这里面会设置网络事件回调函数conn_new-&gt;event_handler
conn *conn_new(const int sfd, enum conn_states init_state,
               const int event_flags, const int read_buffer_size, 
               enum network_transport transport, struct event_base *base) {
    conn *c;

    assert(sfd &gt;= 0 &amp;&amp; sfd &lt; max_fds);
    //直接使用下标
    c = conns[sfd];

    //之前没有哪个连接用过这个sfd值，需要申请一个conn结构体
    if (NULL == c) {
        if (!(c = (conn *)calloc(1, sizeof(conn)))) {
            STATS_LOCK();
            stats.malloc_fails++;
            STATS_UNLOCK();
            fprintf(stderr, &quot;Failed to allocate connection object\n&quot;);
            return NULL;
        }
        MEMCACHED_CONN_CREATE(c);
        //初始化一些成员变量
         c-&gt;sfd = sfd;
        //将这个结构体交由conns数组管理
        conns[sfd] = c;
    }
    ...//初始化另外一些成员变量  
    c-&gt;state = init_state;//值为conn_listening  

    //等同于event_assign，会自动关联current_base。event的回调函数是event_handler  
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);  
    event_base_set(base, &amp;c-&gt;event);  
    c-&gt;ev_flags = event_flags;  

    if (event_add(&amp;c-&gt;event, 0) == -1) {  
        perror(&quot;event_add&quot;);  
        return NULL;  
    }  

    return c;  
}  
</code></pre><p>这个函数最重要的一件事就是为server_socket中建立的socket设置可读事件回调函数event_handler，而且连接的初始状态事conn_listening，这个状态是主线程唯一的状态，下面我们来抽丝剥茧看看event_handler做了什么。</p>
<pre><code>void event_handler(const int fd, const short which, void *arg) {
    conn *c;

    c = (conn *)arg;
    assert(c != NULL);

    c-&gt;which = which;

    /* sanity */
    if (fd != c-&gt;sfd) {
        if (settings.verbose &gt; 0)
            fprintf(stderr, &quot;Catastrophic: event fd doesn&apos;t match conn fd!\n&quot;);
        conn_close(c);
        return;
    }

    drive_machine(c);

    /* wait for next event */
    return;
}
</code></pre><p>它由调用了drive_machine，真是各种调啊，让我们揭开drive_machine的神秘面纱吧。</p>
<pre><code>//主线程接收到客户端连接，通过event_handler触发drive_machine发送信息&quot;c&quot;给工作子线程进行处理
//工作子线程从队列中取出主线程accept的fd,
//thread_libevent_process中取出该CQ_ITEM后，创建新的conn进行事件处理
static void drive_machine(conn *c) {
    bool stop = false;
    int sfd;
    socklen_t addrlen;
    struct sockaddr_storage addr;
    int nreqs = settings.reqs_per_event;//20

    //drive_machine被调用会进行状态判断，并进行一些处理。但也可能发生状态的转换
    //此时需要一个循环，当进行专题转换时，也能处理
    while (!stop) {
        switch(c-&gt;state) {
            case conn_listening: //只有主线程处于该状态
                   addrlen = sizeof(addr);

                sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);                   
                if (settings.maxconns_fast &amp;&amp;
                    stats.curr_conns + stats.reserved_fds &gt;= settings.maxconns - 1) {
                    str = &quot;ERROR Too many open connections\r\n&quot;;
                    res = write(sfd, str, strlen(str));
                    close(sfd);
                    STATS_LOCK();
                    stats.rejected_conns++;
                    STATS_UNLOCK();
                } else {
                    //选定一个worker线程，new一个CQ_ITEM，把这个CQ_ITEM扔给这个线程
                    dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST,
                                 DATA_BUFFER_SIZE, tcp_transport);
                }

            stop = true;
            break;
        }
    }
    return
}
</code></pre><p>drive_machine是一个状态机，包涵了整个网络模型的所有事件状态，这里只是列出了主线程用到的conn_listening状态，dispatch_conn_new函数完成了选定一个worker线程，new一个CQ_ITEM，把这个CQ_ITEM扔给这个线程并通知它，我们来看看它是怎么做到的。</p>
<pre><code>//工作子线程读管道有数据到来，thread_libevent_process从读管道读取到信息
//对应的主线程写管道在dispatch_conn_new或者switch_item_lock_type

//主线程检查到有新的链接到来，则通过管道通知子线程
void dispatch_conn_new(int sfd, enum conn_states init_state, 
                       int event_flags, int read_buffer_size, 
                       enum network_transport transport) {
    CQ_ITEM *item = cqi_new();
    char buf[1];
    //轮询的方式选定一个worker线程
    int tid = (last_thread + 1) % settings.num_threads;

    LIBEVENT_THREAD *thread = threads + tid; //轮询选择由那个工作现场来处理

    last_thread = tid;

    item-&gt;sfd = sfd;
    item-&gt;init_state = init_state;
    item-&gt;event_flags = event_flags;
    item-&gt;read_buffer_size = read_buffer_size;
    item-&gt;transport = transport;
    //把这个item放到选定的worker线程的CQ队列中
    cq_push(thread-&gt;new_conn_queue, item);

    MEMCACHED_CONN_DISPATCH(sfd, thread-&gt;thread_id);
    buf[0] = &apos;c&apos;;
    // 通知worker线程，有新客户单连接到来
    if (write(thread-&gt;notify_send_fd, buf, 1) != 1) {
        perror(&quot;Writing to thread notify pipe&quot;);
    }
}
</code></pre><p>我们可以很清楚的看到，主线程采用轮询的方式选取一个工作线程，调用cq_push将CQ_ITEM放入工作线程的队列中，然后向管道写入一个字符，来唤醒工作线程。<br>至此主线程处理逻辑已经基本完成，由于调用链路较长，我们再重新来简单的梳理下。</p>
<blockquote>
<p>主线程：server_sockets–&gt;server_socket–&gt;conn_new–&gt;event_set(注册socket可读事件回调event_handler)</p>
<p>回调函数：event_handle–&gt;drive_machine–&gt;dispatch_conn_new</p>
</blockquote>
<h2 id="工作线程与CQ队列"><a href="#工作线程与CQ队列" class="headerlink" title="工作线程与CQ队列"></a>工作线程与CQ队列</h2><p>工作线程的初始化在main函数中的thread_init函数中，让我们来看看吧。</p>
<pre><code>//参数nthread是woker线程的数量。main_base则是主线程的event_base
//主线程在main函数调用本函数，创建nthreads个worker线程
void thread_init(int nthreads, struct event_base *main_base) {
    int         i;
    int         power;

    //申请一个CQ_ITEM时需要加锁
    pthread_mutex_init(&amp;cache_lock, NULL);
    pthread_mutex_init(&amp;stats_lock, NULL);

    pthread_mutex_init(&amp;init_lock, NULL);
    pthread_cond_init(&amp;init_cond, NULL);

    pthread_mutex_init(&amp;cqi_freelist_lock, NULL);
    cqi_freelist = NULL;

    item_lock_count = hashsize(power);
    item_lock_hashpower = power;

    //哈希表中段级别的锁。并不是一个桶就对应有一个锁。而是多个桶共用一个锁
    item_locks = calloc(item_lock_count, sizeof(pthread_mutex_t));
    if (! item_locks) {
        perror(&quot;Can&apos;t allocate item locks&quot;);
        exit(1);
    }
    for (i = 0; i &lt; item_lock_count; i++) {
        pthread_mutex_init(&amp;item_locks[i], NULL);
    }
    pthread_key_create(&amp;item_lock_type_key, NULL);
    pthread_mutex_init(&amp;item_global_lock, NULL);

    //申请具有nthreads个元素的LIBEVENT_THREAD数据
    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
    if (! threads) {
        perror(&quot;Can&apos;t allocate thread descriptors&quot;);
        exit(1);
    }
    //主线程对应的
    dispatcher_thread.base = main_base;
    dispatcher_thread.thread_id = pthread_self();

    //子线程的
    for (i = 0; i &lt; nthreads; i++) {
        int fds[2];
        //为每个worker线程分配一个管道，用于通知worker线程
        if (pipe(fds)) {
            perror(&quot;Can&apos;t create notify pipe&quot;);
            exit(1);
        }

        threads[i].notify_receive_fd = fds[0];
        threads[i].notify_send_fd = fds[1];
        //每一个线程配一个event_base，并设置event监听notify_receive_fd的读事件
        //同时还为这个线程分配一个conn_queue队列
        setup_thread(&amp;threads[i]);
        /* Reserve three fds for the libevent base, and two for the pipe */
        stats.reserved_fds += 5;
    }

    /* Create threads after we&apos;ve done all the libevent setup. */
    for (i = 0; i &lt; nthreads; i++) {
         //创建线程，线程函数为worker_libevent，线程参数为&amp;thread[i]
        create_worker(worker_libevent, &amp;threads[i]);
    }

    /* Wait for all the threads to set themselves up before returning. */
    pthread_mutex_lock(&amp;init_lock);
    wait_for_thread_registration(nthreads); //主线程阻塞等待事件到来
    pthread_mutex_unlock(&amp;init_lock);
}
</code></pre><p>代码说明了一切，请注意最后的wait_for_thread_registration函数，主线程将在此阻塞，直到所有的工作线程都初始化完成，我们来看看setup_thread函数。</p>
<pre><code>static void setup_thread(LIBEVENT_THREAD *me) {
    //新建一个event_base
    me-&gt;base = event_init();

    /* Listen for notifications from other threads */
    //监听管道的读端
    event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd,
                EV_READ | EV_PERSIST, thread_libevent_process, me);
    //将event_base和event想关联
    event_base_set(me-&gt;base, &amp;me-&gt;notify_event);

    if (event_add(&amp;me-&gt;notify_event, 0) == -1) {
        fprintf(stderr, &quot;Can&apos;t monitor libevent notify pipe\n&quot;);
        exit(1);
    }
    // 创建一个CQ队列
    me-&gt;new_conn_queue = malloc(sizeof(struct conn_queue));
    if (me-&gt;new_conn_queue == NULL) {
        perror(&quot;Failed to allocate memory for connection queue&quot;);
        exit(EXIT_FAILURE);
    }
    cq_init(me-&gt;new_conn_queue);

    if (pthread_mutex_init(&amp;me-&gt;stats.mutex, NULL) != 0) {
        perror(&quot;Failed to initialize mutex&quot;);
        exit(EXIT_FAILURE);
    }

    me-&gt;suffix_cache = cache_create(&quot;suffix&quot;, SUFFIX_SIZE, sizeof(char*),
                                NULL, NULL);
    if (me-&gt;suffix_cache == NULL) {
        fprintf(stderr, &quot;Failed to create suffix cache\n&quot;);
        exit(EXIT_FAILURE);
    }
}
</code></pre><p>这个函数做的比较重要的事是为管道读端注册可读事件回调thread_libevent_process，用来处理主线程写入的字符，创建了CQ队列，让我们来看看事件回调函数。</p>
<pre><code>//工作子线程读管道有数据到来，thread_libevent_process从读管道读取到信息
//对应的主线程写管道在dispatch_conn_new或者switch_item_lock_type
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    if (read(fd, buf, 1) != 1)
       switch (buf[0]) {
            case &apos;c&apos;: //dispatch_conn_new
            //从CQ队列中读取一个item，因为是pop所以读取后，CQ队列会把这个item从队列中删除
                item = cq_pop(me-&gt;new_conn_queue);

                if (NULL != item) {
            //为sfd分配一个conn结构体，并且为这个sfd建立一个event，然后让base监听这个event
            //这个sfd的事件回调函数是event_handler
                    conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, 
                    item-&gt;event_flags, item-&gt;read_buffer_size, 
                    item-&gt;transport, me-&gt;base);
                } else {
                    c-&gt;thread = me;
                }
                cqi_free(item);
                break;
            //switch_item_lock_type触发走到这里
            /* we were told to flip the lock type and report in */
            case &apos;l&apos;: //参考switch_item_lock_type //切换item到段级别
            //唤醒睡眠在init_cond条件变量上的迁移线程
                me-&gt;item_lock_type = ITEM_LOCK_GRANULAR;
                register_thread_initialized();
                break;
            case &apos;g&apos;://切换item锁到全局级别
                me-&gt;item_lock_type = ITEM_LOCK_GLOBAL;
                register_thread_initialized();
                break;
        }
    }
}
</code></pre><p>我们可以看到工作线程会从自己的队列里面取出主线程push的CQ_ITEM，然后调用conn_new同样注册事件回调event_handler，最后会走到drive_machine，这个逻辑和主线程是一样的。让我们再看看thread_init里面的create_worker函数。</p>
<pre><code>static void create_worker(void *(*func)(void *), void *arg) {
    pthread_t       thread;
    pthread_attr_t  attr;
    int             ret;

    pthread_attr_init(&amp;attr);

    if ((ret = pthread_create(&amp;thread, &amp;attr, func, arg)) != 0) {
        fprintf(stderr, &quot;Can&apos;t create thread: %s\n&quot;,
            strerror(ret));
        exit(1);
    }
}
</code></pre><p>这个函数就是真正的创建工作线程的地方，在所有的回调函数都设置完毕之后就可以启动工作线程了，我们还是来简单的梳理下调用过程。</p>
<blockquote>
<p>主线程：thread_init–&gt;setup_thread(注册可读事件回调thread_libevent_process)</p>
<p>回调函数：thread_libevent_process–&gt;conn_new–&gt;event_handler–&gt;drive_machine</p>
<p>主线程：thread_init–&gt;create_worker–&gt;pthread_create</p>
</blockquote>
<p>这样整个memcached的网络部分就算是过了一遍，如果想详细的了解还得看代码，再次感谢文章开头所参考的博客和代码的作者。</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/memcached-网络/" rel="tag">#memcached 网络</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/28/memcached源码剖析之hash表及基本操作/" rel="prev" title="memcached源码剖析之hash表及基本操作">
                memcached源码剖析之hash表及基本操作 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Übersicht
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="chenshuiyin" />
          <p class="site-author-name" itemprop="name">chenshuiyin</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#memcached网络模型图"><span class="nav-number">1.</span> <span class="nav-text">memcached网络模型图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CQ队列及基本操作"><span class="nav-number">2.</span> <span class="nav-text">CQ队列及基本操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#主线程与CQ队列"><span class="nav-number">3.</span> <span class="nav-text">主线程与CQ队列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#工作线程与CQ队列"><span class="nav-number">4.</span> <span class="nav-text">工作线程与CQ队列</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenshuiyin</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  
  

  

  

</body>
</html>
