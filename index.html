<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Practice makes perfect">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Practice makes perfect">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Practice makes perfect">
<meta name="twitter:description">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> Practice makes perfect </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Practice makes perfect</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/28/memcached源码剖析之内存管理/" itemprop="url">
                  memcached源码剖析之内存管理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-06-28T00:04:54+08:00" content="2016-06-28">
              2016-06-28
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>   参考博客：<br><a href="http://blog.csdn.net/luotuo44/article/details/42737181" target="_blank" rel="external">http://blog.csdn.net/luotuo44/article/details/42737181</a><br><a href="http://brionas.github.io/2014/01/06/memcached-manage/" target="_blank" rel="external">http://brionas.github.io/2014/01/06/memcached-manage/</a></p>
<p>   代码来自：<br><a href="https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22" target="_blank" rel="external">https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22</a></p>
</blockquote>
<p>此博客完全是自己在学习过程中的笔记，没有任何商业用途</p>
<h2 id="内存管理系统简介"><a href="#内存管理系统简介" class="headerlink" title="内存管理系统简介"></a>内存管理系统简介</h2><p>内存管理算法比较经典的有buddy分配算法和slab分配算法，关于buddy分配算法可以参考这两篇博客 </p>
<blockquote>
<p><a href="http://coolshell.cn/articles/10427.html" target="_blank" rel="external">http://coolshell.cn/articles/10427.html</a><br><a href="http://blog.codingnow.com/2011/12/buddy_memory_allocation.html" target="_blank" rel="external">http://blog.codingnow.com/2011/12/buddy_memory_allocation.html</a></p>
</blockquote>
<p>slab算法就是本文要介绍的，memcached所采用的内存分配算法。Linux2.4也采用了slab分配器算法，该算法比传统的分配器算法有更好性能和内存利用率。</p>
<ol>
<li>小对象的申请和释放通过slab分配器来管理。</li>
<li>slab分配器有一组高速缓存，每个高速缓存保存同一种对象类型，如i节点缓存、PCB缓存等。</li>
<li>内核从它们各自的缓存种分配和释放对象。</li>
<li>每种对象的缓存区由一连串slab构成，每个slab由一个或者多个连续的物理页面组成。这些页面种包含了已分配的缓存对象，也包含了空闲对象。</li>
</ol>
<p>具体来说就是slab算法采用固定内存大小，虽然是固定内存大小，但是每种内存大小有一个集合，例如，有1k、4k、8k、16k大小的内存分别有一个集合。如下图所示。<br><img src="https://github.com/chenshuiyin/ImageCache/raw/master/slab.png" alt="memcached网络"></p>
<p>在阅读源码之前我们需要明确几个概念：</p>
<ol>
<li>Page  ：分配给Slab的内存空间，默认是1MB。分配给Slab之后根据slab的大小切分成chunk。</li>
<li>Chunk：用于缓存记录的内存空间。</li>
<li>Slab Class：特定大小的chunk的组。</li>
</ol>
<p>下面这张图基本涵盖了slab分配机制。<br><img src="https://github.com/chenshuiyin/ImageCache/raw/master/slab1.png" alt="memcached网络"><br>我们结合上图逐步来了解源码吧。先来看下图中对应的结构体定义。</p>
<h3 id="slabclass-t结构体"><a href="#slabclass-t结构体" class="headerlink" title="slabclass_t结构体"></a>slabclass_t结构体</h3><pre><code>//slabs.c文件
typedef struct {
    unsigned int size;//slab分配器分配的item的大小     
    unsigned int perslab; //每一个slab分配器能分配多少个item

    void *slots;  //指向空闲item链表
    unsigned int sl_curr;     //空闲item的个数

    //这个是已经分配了内存的slabs个数。list_size是这个slabs数组(slab_list)的大小    
    unsigned int slabs; //本slabclass_t可用的slab分配器个数   
    //slab数组，数组的每一个元素就是一个slab分配器，这些分配器都分配相同尺寸的内存
    void **slab_list;     
    unsigned int list_size; //slab数组的大小, list_size &gt;= slabs

    //用于reassign，指明slabclass_t中的哪个块内存要被其他slabclass_t使用
    unsigned int killing; 

    size_t requested; //本slabclass_t分配出去的字节数
} slabclass_t;




//数组元素虽然有MAX_NUMBER_OF_SLAB_CLASSES个，但实际上并不是全部都使用的。
//实际使用的元素个数由power_largest指明
static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];//201
static int power_largest;//slabclass数组中,已经使用了的元素个数.
</code></pre><p>我们需要注意的是由于slab_list是一个空间大小固定的指针数组，而list_size是这个指针数组的大小，slabs代表已经分配出去的slabs数，list_size则代表总共有多少个slabs数，所以当slabs等于list_size的时候代表这个slab_list已经满了，得增大空间。所以每次涉及到内存的改变都会去调用grow_slab_list看是否需要扩充slab_list的大小，memcached启动的时候会定义几种内存大小集合slabclass。</p>
<h3 id="item结构体"><a href="#item结构体" class="headerlink" title="item结构体"></a>item结构体</h3><pre><code>typedef struct _stritem {
    //next指针，用于LRU链表
    struct _stritem *next;
    //prev指针，用于LRU链表
    struct _stritem *prev;
    //h_next指针，用于哈希表的冲突链
    struct _stritem *h_next;    /* hash chain next */
    //最后一次访问时间。绝对时间  lru队列里面的item是根据time降序排序的
    rel_time_t      time;       /* least recent access */
    //过期失效时间，绝对时间 一个item在两种情况下会过期失效：
    //1.item的exptime时间戳到了。2.用户使用flush_all命令将全部item变成过期失效的
    rel_time_t      exptime;    /* expire time */
    //本item存放的数据的长度   nbytes是算上了\r\n字符的，见do_item_alloc外层   
    //参考item_make_header中的
    int             nbytes;     /* size of data */

    /*
    可以看到，这是因为减少一个item的引用数可能要删除这个item。为什么呢？考虑这样的情景，线程        A因为要读一个item而增加了这个item的
    引用计数，此时线程B进来了，它要删除这个item。这个删除命令是肯定会执行的，而不是说这个        item被别的线程引用了就不执行删除命令。
    但又肯定不能马上删除，因为线程A还在使用这个item，此时memcached就采用延迟删除的做法。线程        B执行删除命令时减多一次item的引用数，
    使得当线程A释放自己对item的引用后，item的引用数变成0。此时item就被释放了(归还给slab分配        器)。
    */

    //本item的引用数 在使用do_item_remove函数向slab归还item时，会先测试这个item的引用数是否等于0。
    //引用数可以简单理解为是否有worker线程在使用这个item   记录这个item被引用(被worker线程占用)的总数
    //参考http://blog.csdn.net/luotuo44/article/details/42913549
    unsigned short  refcount;  //在do_item_link中增加  //新开盘的默认初值为1
    //后缀长度，  (nkey + *nsuffix + nbytes)中的nsuffix  参考item_make_header中的
    uint8_t         nsuffix;    /* length of flags-and-length string */
    //item的属性  是否使用cas，settings.use_cas   // item的三种flag: ITEM_SLABBED,         ITEM_LINKED,ITEM_CAS
    uint8_t         it_flags;   /* ITEM_* above */
    //该item是从哪个slabclass获取得到
    uint8_t         slabs_clsid;/* which slab class we&apos;re in */
    //键值的长度 实际上真实用到的内存是nkey+1,见do_item_alloc  参考item_make_header中的
    uint8_t         nkey;       /* key length, w/terminating null and padding */


    union {
        //ITEM_set_cas  只有在开启settings.use_cas才有用
        //ITEM_set_cas   get_cas_id  ITEM_get_cas  
        //每次读取完数据部分后，存储到item中后，stored的时候都会调用do_store_item自增
        uint64_t cas; //参考process_update_command中的req_cas_id,实际是从客户端的set等命令中获取到的
        char end;
    } data[];
    //+ DATA 这后面的就是实际数据 (nkey + *nsuffix + nbytes) 参考item_make_header
} item;
</code></pre><h3 id="LRU链"><a href="#LRU链" class="headerlink" title="LRU链"></a>LRU链</h3><pre><code>//指向每一个LRU队列头  
//LRU队列可以参考http://blog.csdn.net/luotuo44/article/details/42869325
//排在队列前面的表示最近被访问的key，排在head后面的则表示最近没访问该key，
//因为每次访问key都会把key-value对应的item取出来放到head头部，见do_item_update
//lru队列里面的item是根据time降序排序的

//LRU真正的淘汰机制实现过程在do_item_alloc-&gt;it = slabs_alloc(ntotal, id)) == NULL 
//既没有过期的item，获取新item又失败了，则进行LRU淘汰机制
static item *heads[LARGEST_ID]; //LRU链首指针,  每个classid一个LRU链
//指向每一个LRU队列尾
static item *tails[LARGEST_ID]; //LRU链尾指针，每个classid一个LRU链
</code></pre><p>LRU链是memcached用来淘汰内存中过期的item的，每个slabclass都保存着一个LRU队列，而head[i]和tail[i]则就是id为i的slabclass LRU队列的头部和尾部，也是采用的头插法，即越靠近head的item越是最近访问到的，所以每次访问到一个item会将它在LRU链中移动到靠前的位置，但是为来避免某一个item由于被频繁的访问引起频繁的在LRU中移动，memcached设置了一个时间间隔来控制它。</p>
<h3 id="slab初始化"><a href="#slab初始化" class="headerlink" title="slab初始化"></a>slab初始化</h3><p>slab初始化的整个流程可以看下面这张图。<br><img src="https://github.com/chenshuiyin/ImageCache/raw/master/slab2.png" alt="memcached网络"></p>
<pre><code>//slabs.c文件
static size_t mem_limit = 0;//用户设置的内存最大限制
static size_t mem_malloced = 0;

//如果程序要求预先分配内存，而不是到了需要的时候才分配内存，那么
//mem_base就指向那块预先分配的内存.
//mem_current指向还可以使用的内存的开始位置
//mem_avail指明还有多少内存是可以使用的
static void *mem_base = NULL;
static void *mem_current = NULL;
static size_t mem_avail = 0;


//参数factor是扩容因子，默认值是1.25
void slabs_init(const size_t limit, const double factor, const bool prealloc) {
    int i = POWER_SMALLEST - 1;

    //settings.chunk_size默认值为48，可以在启动memcached的时候通过-n选项设置
    //size由两部分组成: item结构体本身 和 这个item对应的数据
    //这里的数据也就是set、add命令中的那个数据.后面的循环可以看到这个size变量会
    //根据扩容因子factor慢慢扩大，所以能存储的数据长度也会变大的
    unsigned int size = sizeof(item) + settings.chunk_size;

    mem_limit = limit;//用户设置或者默认的内存最大限制

    //用户要求预分配一大块的内存，以后需要内存，就向这块内存申请。
    if (prealloc) {//默认值为false
        mem_base = malloc(mem_limit);
        if (mem_base != NULL) {
            mem_current = mem_base;
            mem_avail = mem_limit;
        } else {
            fprintf(stderr, &quot;Warning: Failed to allocate requested memory in&quot;
                    &quot; one large chunk.\nWill allocate in smaller chunks\n&quot;);
        }
    }

    //初始化数组，这个操作很重要，数组中所有元素的成员变量值都为0了
    memset(slabclass, 0, sizeof(slabclass));

    //slabclass数组中的第一个元素并不使用
    //settings.item_size_max是memcached支持的最大item尺寸，默认为1M(也就是网上
    //所说的memcached存储的数据最大为1MB)。
    while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) {
        /* Make sure items are always n-byte aligned */
        if (size % CHUNK_ALIGN_BYTES)//8字节对齐
            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);

        //这个slabclass的slab分配器能分配的item大小
        slabclass[i].size = size;
        //这个slabclass的slab分配器最多能分配多少个item(也决定了最多分配多少内存)
        slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
        size *= factor;//扩容
    }

    //最大的item
    power_largest = i;
    slabclass[power_largest].size = settings.item_size_max;
    slabclass[power_largest].perslab = 1;

    ...


    if (prealloc) {//预分配内存
        slabs_preallocate(power_largest);
    }
}
</code></pre><p>如果用户需要预分配一部分内存，就首先调用malloc分配内存，然后初始化slabclass，由于调用malloc申请的是一块连续的内存被称作页（page），所以需要调用slabs_preallocate将这个页切割成对应大小的chunk。</p>
<pre><code>//参数值为使用到的slabclass数组元素个数
//为slabclass数组的每一个元素(使用到的元素)分配内存
static void slabs_preallocate (const unsigned int maxslabs) {
    int i;
    unsigned int prealloc = 0;

    //遍历slabclass数组
    for (i = POWER_SMALLEST; i &lt;= POWER_LARGEST; i++) {
        if (++prealloc &gt; maxslabs)//当然是只遍历使用了的数组元素
            return;
        if (do_slabs_newslab(i) == 0) {//为每一个slabclass_t分配一个内存页
            //如果分配失败，将退出程序.因为这个预分配的内存是后面程序运行的基础
            //如果这里分配失败了，后面的代码无从执行。所以就直接退出程序。
            exit(1);
        }
    }

}


//slabclass_t中slab的数目是慢慢增多的。该函数的作用就是为slabclass_t申请多一个slab
//参数id指明是slabclass数组中的那个slabclass_t
static int do_slabs_newslab(const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    //settings.slab_reassign的默认值为false，这里就采用false。
    int len = settings.slab_reassign ? settings.item_size_max
                : p-&gt;size * p-&gt;perslab;//其积 &lt;= settings.item_size_max
    char *ptr;

    //mem_malloced的值通过环境变量设置，默认为0
    //增长slab_list(失败返回0)。一般都会成功,除非无法分配内存
    if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) ||
        (grow_slab_list(id) == 0) ||
        ((ptr = memory_allocate((size_t)len)) == 0)) {
        //分配len字节内存(也就是一个页)

        return 0;
    }

    memset(ptr, 0, (size_t)len);//清零内存块是必须的
    //将这块内存切成一个个的item，当然item的大小有id所控制
    split_slab_page_into_freelist(ptr, id);

    //将分配得到的内存页交由slab_list掌管
    p-&gt;slab_list[p-&gt;slabs++] = ptr;
    mem_malloced += len;

    return 1;
}
</code></pre><p>do_slabs_newslab调用了grow_slab_list，memory_allocate，split_slab_page_into_freelist分别对应增长slabclass的大小，申请大小为len字节的内存和负责把申请到的内存切分成多个item。</p>
<pre><code>//增加slab_list成员指向的内存，也就是增大slab_list数组。使得可以有更多的slab分配器
//除非内存分配失败，否则都是返回1,无论是否真正增大了
static int grow_slab_list (const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    if (p-&gt;slabs == p-&gt;list_size) {//用完了之前申请到的slab_list数组的所有元素
        size_t new_size =  (p-&gt;list_size != 0) ? p-&gt;list_size * 2 : 16;
        void *new_list = realloc(p-&gt;slab_list, new_size * sizeof(void *));
        if (new_list == 0) return 0;
        p-&gt;list_size = new_size;
        p-&gt;slab_list = new_list;
    }
    return 1;
}


//申请分配内存，如果程序是有预分配内存块的，就向预分配内存块申请内存
//否则调用malloc分配内存
static void *memory_allocate(size_t size) {
    void *ret;

    //如果程序要求预先分配内存，而不是到了需要的时候才分配内存，那么
    //mem_base就指向那块预先分配的内存.
    //mem_current指向还可以使用的内存的开始位置
    //mem_avail指明还有多少内存是可以使用的
    if (mem_base == NULL) {//不是预分配内存
        /* We are not using a preallocated large memory chunk */
        ret = malloc(size);
    } else {
        ret = mem_current;

        //在字节对齐中，最后几个用于对齐的字节本身就是没有意义的(没有被使用起来)
        //所以这里是先计算size是否比可用的内存大，然后才计算对齐

        if (size &gt; mem_avail) {//没有足够的可用内存
            return NULL;
        }

        //现在考虑对齐问题，如果对齐后size 比mem_avail大也是无所谓的
        //因为最后几个用于对齐的字节不会真正使用
        /* mem_current pointer _must_ be aligned!!! */
        if (size % CHUNK_ALIGN_BYTES) {//字节对齐.保证size是CHUNK_ALIGN_BYTES (8)的倍数
            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        }


        mem_current = ((char*)mem_current) + size;
        if (size &lt; mem_avail) {
            mem_avail -= size;
        } else {//此时，size比mem_avail大也无所谓
            mem_avail = 0;
        }
    }

    return ret;
}


//将ptr指向的内存页划分成一个个的item
static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    int x;
    for (x = 0; x &lt; p-&gt;perslab; x++) {
        //将ptr指向的内存划分成一个个的item.一共划成perslab个
        //并将这些item前后连起来。
        //do_slabs_free函数本来是worker线程向内存池归还内存时调用的。但在这里
        //新申请的内存也可以当作是向内存池归还内存。把内存注入内存池中
        do_slabs_free(ptr, 0, id);
        ptr += p-&gt;size;//size是item的大小
    }
}


static void do_slabs_free(void *ptr, const size_t size, unsigned int id) {
    slabclass_t *p;
    item *it;

    assert(((item *)ptr)-&gt;slabs_clsid == 0);
    assert(id &gt;= POWER_SMALLEST &amp;&amp; id &lt;= power_largest);
    if (id &lt; POWER_SMALLEST || id &gt; power_largest)
        return;

    p = &amp;slabclass[id];

    it = (item *)ptr;
    //为item的it_flags添加ITEM_SLABBED属性，标明这个item是在slab中没有被分配出去
    it-&gt;it_flags |= ITEM_SLABBED;

    //由split_slab_page_into_freelist调用时，下面4行的作用是
    //让这些item的prev和next相互指向，把这些item连起来.
    //当本函数是在worker线程向内存池归还内存时调用，那么下面4行的作用是,
    //使用链表头插法把该item插入到空闲item链表中。
    it-&gt;prev = 0;
    it-&gt;next = p-&gt;slots;
    if (it-&gt;next) it-&gt;next-&gt;prev = it;
    p-&gt;slots = it;//slot变量指向第一个空闲可以使用的item

    p-&gt;sl_curr++;//空闲可以使用的item数量
    p-&gt;requested -= size;//减少这个slabclass_t分配出去的字节数
    return;
}
</code></pre><p>当worker线程向内存池归还内存时会调用do_slabs_free函数，采用头插法插入到slots的头部。</p>
<h3 id="申请内存"><a href="#申请内存" class="headerlink" title="申请内存"></a>申请内存</h3><p>申请内存对应的流程如下图。</p>
<p><img src="https://github.com/chenshuiyin/ImageCache/raw/master/slab3.png" alt="memcached网络"></p>
<pre><code>unsigned int slabs_clsid(const size_t size) {//返回slabclass索引下标值
    int res = POWER_SMALLEST;//res的初始值为1

    //返回0表示查找失败，因为slabclass数组中，第一个元素是没有使用的
    if (size == 0)
        return 0;

    //因为slabclass数组中各个元素能分配的item大小是升序的
    //所以从小到大直接判断即可在数组找到最小但又能满足的元素
    while (size &gt; slabclass[res].size)
        if (res++ == power_largest)     /* won&apos;t fit in the biggest slab */
            return 0;
    return res;
}


//向slabclass申请一个item。在调用该函数之前，已经调用slabs_clsid函数确定
//本次申请是向哪个slabclass_t申请item了，参数id就是指明是向哪个slabclass_t
//申请item。如果该slabclass_t是有空闲item，那么就从空闲的item队列中分配一个
//如果没有空闲item，那么就申请一个内存页。再从新申请的页中分配一个item
//返回值为得到的item，如果没有内存了，返回NULL
static void *do_slabs_alloc(const size_t size, unsigned int id) {
    slabclass_t *p;
    void *ret = NULL;
    item *it = NULL;

    if (id &lt; POWER_SMALLEST || id &gt; power_largest) {//下标越界
           MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
        return NULL;
    }

    p = &amp;slabclass[id];
    assert(p-&gt;sl_curr == 0 || ((item *)p-&gt;slots)-&gt;slabs_clsid == 0);

    //如果p-&gt;sl_curr等于0，就说明该slabclass_t没有空闲的item了。
    //此时需要调用do_slabs_newslab申请一个内存页
    if (! (p-&gt;sl_curr != 0 || do_slabs_newslab(id) != 0)) {
        //当p-&gt;sl_curr等于0并且do_slabs_newslab的返回值等于0时，进入这里
        /* We don&apos;t have more memory available */
        ret = NULL;
    } else if (p-&gt;sl_curr != 0) {
    //除非do_slabs_newslab调用失败，否则都会来到这里.无论一开始sl_curr是否为0。
    //p-&gt;slots指向第一个空闲的item，此时要把第一个空闲的item分配出去

        /* return off our freelist */
        it = (item *)p-&gt;slots;
        p-&gt;slots = it-&gt;next;//slots指向下一个空闲的item
        if (it-&gt;next) it-&gt;next-&gt;prev = 0;
        p-&gt;sl_curr--;//空闲数目减一
        ret = (void *)it;
    }

    if (ret) {
        p-&gt;requested += size;//增加本slabclass分配出去的字节数
    }

    return ret;
}
</code></pre><p>当向内存池申请内存的时候，首先需要知道申请内存大小对应的slabclass的下标也就是slabs_clsid函数，在do_slabs_alloc函数中如果对应的slabclass有空闲的item，那么就直接将之分配出去。否则就需要调用do_slabs_newslab扩充slab得到一些空闲的item然后分配出去。这样整个内存管理系统就说完了，涉及到具体的细节也还是得深入到代码中去。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/28/memcached源码剖析之hash表及基本操作/" itemprop="url">
                  memcached源码剖析之hash表及基本操作
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-06-28T00:03:20+08:00" content="2016-06-28">
              2016-06-28
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>   参考博客：<br><a href="http://blog.csdn.net/column/details/memcached-src.html" target="_blank" rel="external">http://blog.csdn.net/column/details/memcached-src.html</a></p>
<p>   代码来自：<br><a href="https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22" target="_blank" rel="external">https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22</a></p>
</blockquote>
<p>此博客完全是自己在学习过程中的笔记，没有任何商业用途</p>
<h2 id="Hash表的定义"><a href="#Hash表的定义" class="headerlink" title="Hash表的定义"></a>Hash表的定义</h2><p>memcached的hash函数根据配置可以使用jenkins_hash或者murmur3哈希，数据结构定义如下。</p>
<pre><code>unsigned int hashpower = HASHPOWER_DEFAULT;

//哈希表数组指针
static item** primary_hashtable = 0;

//当hash扩展的时候，用于存放旧的hashtable
static item** old_hashtable = 0;

static unsigned int hash_items = 0;

//hash扩展的时候置true
static bool expanding = false; 

static bool started_expanding = false;
</code></pre><p>源码中底数为2，指数为hashpower，这样一个幂指数用来表示hash表的大小，之所以这样表示是为了取模的时候方便计算，因为 a % (2^n) 等价于 a &amp; (2^n - 1)，另外需要说明的一点是memecached使用链地址来解决冲突问题。</p>
<h3 id="Hash表初始化"><a href="#Hash表初始化" class="headerlink" title="Hash表初始化"></a>Hash表初始化</h3><pre><code>//默认参数为0.本函数由main函数调用，参数的默认值为0
void assoc_init(const int hashtable_init) {
    if (hashtable_init) {
        hashpower = hashtable_init;
    }
    //因为哈希表会慢慢增大，所以要使用动态内存分配。
    //哈希表存储的数据是一个指针，这样更省空间。
    //hashsize(hashpower)就是哈希表的长度了
    primary_hashtable = calloc(hashsize(hashpower),
                               sizeof(void *));
    if (! primary_hashtable) {
        fprintf(stderr, &quot;Failed to init hashtable.\n&quot;);
        exit(EXIT_FAILURE);
        //哈希表是memcached工作的基础，如果失败只能退出运行
    }
    STATS_LOCK();
    stats.hash_power_level = hashpower;
    stats.hash_bytes = hashsize(hashpower) * sizeof(void *);
    STATS_UNLOCK();
}
</code></pre><h3 id="Hash表查找"><a href="#Hash表查找" class="headerlink" title="Hash表查找"></a>Hash表查找</h3><pre><code>//由于哈希值只能确定是在哈希表中的哪个桶(bucket)，但一个桶里      面是有一条冲突链的
//此时需要用到具体的键值遍历并一一比较冲突链上的所有节点。虽然      key是以&apos;\0&apos;结尾的
//字符串，但调用strlen还是有点耗时(需要遍历键值字符串)。所以      需要另外一个参数nkey
//指明这个key的长度用于快速查找该key对应的item
item *assoc_find(const char *key, 
                const size_t nkey, 
                const uint32_t hv) {
    item *it;
    unsigned int oldbucket;

    if (expanding &amp;&amp;
        (oldbucket = 
        (hv &amp; hashmask(hashpower - 1))) &gt;=     
        expand_bucket) {
        it = old_hashtable[oldbucket];
    } else {
        //由哈希值判断这个key是属于哪个桶的
        it = primary_hashtable[hv &amp;
                                  hashmask(hashpower)];
    }    

    //到这里，已经确定这个key是属于哪个桶的，遍历对应桶的冲突链即可
    item *ret = NULL;
    int depth = 0;
    while (it) {
         //长度相同的情况下才调用memcmp比较，更高效
        if ((nkey == it-&gt;nkey) &amp;&amp; 
            (memcmp(key, ITEM_key(it), nkey) == 0)) {
            ret = it;
            break;
        }
        it = it-&gt;h_next;
        ++depth;
    }
    MEMCACHED_ASSOC_FIND(key, nkey, depth);
    return ret;
}
</code></pre><h3 id="Hash表插入"><a href="#Hash表插入" class="headerlink" title="Hash表插入"></a>Hash表插入</h3><p> 因为插入的时候可能哈希表正在扩展，所以插入的时候要面临一个选择：插入到新表还是旧表？memcached的做法是：当item对应在旧表中的桶还没被迁移到新表的话，就插入到旧表，否则插入到新表。</p>
<pre><code>//hv是这个item键值的哈希值，用于快速查找该key对应的item
int assoc_insert(item *it, const uint32_t hv) {
    unsigned int oldbucket; 

    //使用头插法，插入一个item
    //第一次看本函数，直接看else部分
    if (expanding &amp;&amp;
        (oldbucket = (hv &amp; hashmask(hashpower - 1))) 
         &gt;= expand_bucket) {
            it-&gt;h_next = old_hashtable[oldbucket];
            old_hashtable[oldbucket] = it;
    } else {
           //使用头插法插入哈希表中
        it-&gt;h_next = primary_hashtable[hv &amp; 
                                hashmask(hashpower)];
        primary_hashtable[hv &amp; hashmask(hashpower)] = it;
    }

    hash_items++;//哈希表的item数量加一
    if (! expanding &amp;&amp; hash_items &gt; 
       (hashsize(hashpower) * 3) / 2) {
        assoc_start_expand();
    }

    MEMCACHED_ASSOC_INSERT(ITEM_key(it), 
                           it-&gt;nkey, hash_items);
    return 1;
}
</code></pre><h3 id="Hash表删除"><a href="#Hash表删除" class="headerlink" title="Hash表删除"></a>Hash表删除</h3><pre><code>void assoc_delete(const char *key, 
                  const size_t nkey, 
                  const uint32_t hv) {
    //得到前驱结点的h_next成员地址
    item **before = _hashitem_before(key, nkey, hv);

    if (*before) {//查找成功
        item *nxt;
        hash_items--;

        MEMCACHED_ASSOC_DELETE(key, nkey, hash_items);

        //因为before是一个二级指针，
        //其值为所查找item的前驱item的h_next成员地址
        //所以*before指向的是所查找的item。
        //因为before是一个二级指针，所以*before
        //作为左值时，可以给h_next成员变量赋值。
        //所以下面三行代码是使得删除中间的item后，
        //前后的item还能连接起来。

        nxt = (*before)-&gt;h_next;
        (*before)-&gt;h_next = 0;   
        *before = nxt;
        return;
    }

    assert(*before != 0);
}


//查找item。返回前驱结点的h_next成员地址，
//如果查找失败那么就返回冲突链中最后
//一个节点的h_next成员地址。因为最后一个节点的h_next的值
//为NULL。通过对返回值使用*运算即可知道有没有查找成功
static item** _hashitem_before (const char *key, 
               const size_t nkey, const uint32_t hv) {
    item **pos;
    unsigned int oldbucket;

    //正在扩展哈希表
    if (expanding &amp;&amp; (oldbucket = 
        (hv &amp; hashmask(hashpower - 1))) &gt;= 
        expand_bucket) {
        pos = &amp;old_hashtable[oldbucket];
    } else {
        //找到哈希表中对应的桶的位置
        pos = &amp;primary_hashtable[hv &amp; 
                                hashmask(hashpower)];
    }

    //遍历桶的冲突链查找item
    while (*pos &amp;&amp; ((nkey != (*pos)-&gt;nkey) || 
           memcmp(key, ITEM_key(*pos), nkey))) {
        pos = &amp;(*pos)-&gt;h_next;
    }
    //*pos就可以知道有没有查找成功。
    //如果*pos等于NULL那么查找失败，否则查找成功
    return pos;
}
</code></pre><h3 id="Hash表的扩展"><a href="#Hash表的扩展" class="headerlink" title="Hash表的扩展"></a>Hash表的扩展</h3><p>当哈希表中item的数量达到了哈希表表长的1.5倍时，那么就会扩展哈希表增大哈希表的表长，这样就涉及到数据的迁移，由于数据迁移是一个浩大的工作，需要一个专门的线程maintenance_thread来进行。</p>
<pre><code>//main函数会调用本函数，启动数据迁移线程
int start_assoc_maintenance_thread() {
    int ret;
    char *env = getenv(&quot;MEMCACHED_HASH_BULK_MOVE&quot;);
    if (env != NULL) {
        //hash_bulk_move的作用在后面会说到。
        //这里是通过环境变量给hash_bulk_move赋值
        hash_bulk_move = atoi(env);
        if (hash_bulk_move == 0) {
            hash_bulk_move = DEFAULT_HASH_BULK_MOVE;
        }
    }
    if ((ret = pthread_create(&amp;maintenance_tid, NULL,
              assoc_maintenance_thread, NULL)) != 0) {
        fprintf(stderr, &quot;Can&apos;t create thread: %s\n&quot;,
                         strerror(ret));
        return -1;
    }
    return 0;
}
</code></pre><p>迁移线程被创建后会进入休眠状态(通过等待条件变量)，当worker线程插入item后，发现需要扩展哈希表就会调用assoc_start_expand函数唤醒这个迁移线程。</p>
<pre><code>static bool started_expanding = false;

//assoc_insert函数会调用本函数，当item数量到了哈希表表长的1.5倍才会调用的
static void assoc_start_expand(void) {
    if (started_expanding)
        return;
    started_expanding = true;
    pthread_cond_signal(&amp;maintenance_cond);
}


static bool expanding = false;
static volatile int do_run_maintenance_thread = 1;
static void *assoc_maintenance_thread(void *arg) {

    //do_run_maintenance_thread是全局变量，初始值为1，在stop_assoc_maintenance_thread
    //函数中会被赋值0，终止迁移线程
    while (do_run_maintenance_thread) {
        int ii = 0;

        //上锁
        item_lock_global();
        mutex_lock(&amp;cache_lock);

        ...//进行item迁移

        //遍历完就释放锁
        mutex_unlock(&amp;cache_lock);
        item_unlock_global();


        if (!expanding) {//不需要迁移数据(了)。
            mutex_lock(&amp;cache_lock);
            started_expanding = false; //重置

            //挂起迁移线程，直到worker线程插入数据后发现item数量已经到了1.5倍哈希表大小，
            //此时调用worker线程调用assoc_start_expand函数，该函数会调用pthread_cond_signal
            //唤醒迁移线程
            pthread_cond_wait(&amp;maintenance_cond, 
                              &amp;cache_lock);
            mutex_unlock(&amp;cache_lock);

            ...

            mutex_lock(&amp;cache_lock);
            //申请更大的哈希表,并将expanding设置为true
            assoc_expand();
            mutex_unlock(&amp;cache_lock);
        }
    }
    return NULL;
}
</code></pre><p>为了不影响worker thread，memcached采用逐步迁移，每次都只迁移hash_bulk_move个桶，逐步迁移的具体做法是，调用assoc_expand函数申请一个新的更大的哈希表，每次迁移都需要给整个hash表上锁。</p>
<pre><code>static void assoc_expand(void) {
    old_hashtable = primary_hashtable;

    //申请一个新哈希表，并用old_hashtable指向旧哈希表
    primary_hashtable = calloc(hashsize(hashpower + 
                                1), sizeof(void *));
    if (primary_hashtable) {
        hashpower++;
        expanding = true;//标明已经进入扩展状态
        expand_bucket = 0;//从0号桶开始数据迁移
    } else {
        primary_hashtable = old_hashtable;
    }
}
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/28/memcached源码剖析之网络/" itemprop="url">
                  memcached源码剖析之网络
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-06-28T00:03:20+08:00" content="2016-06-28">
              2016-06-28
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>   参考博客：<br><a href="http://blog.csdn.net/column/details/memcached-src.html" target="_blank" rel="external">http://blog.csdn.net/column/details/memcached-src.html</a></p>
<p>   代码来自：<br><a href="https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22" target="_blank" rel="external">https://github.com/y123456yz/Reading-and-comprehense-memcached-1.4.22</a></p>
</blockquote>
<p>此博客完全是自己在学习过程中的笔记，没有任何商业用途</p>
<h2 id="memcached网络模型图"><a href="#memcached网络模型图" class="headerlink" title="memcached网络模型图"></a>memcached网络模型图</h2><p><img src="https://github.com/chenshuiyin/ImageCache/raw/master/memcached.png" alt="memcached网络"></p>
<p>简单叙述就是memcached网络框架分为主线程（main thread）和工作线程（worker thread），主线程负责连接的建立，工作线程负责请求消息体的接收、处理、响应以及连接的关闭，这种没有脱离《Unix网络编程》里面所列出的几种设计范式（各种模型的使用场景可以参数此书），实际上就是下图中的第8种。</p>
<p><img src="https://github.com/chenshuiyin/ImageCache/raw/master/网络模型.png" alt="memcached网络"></p>
<h2 id="CQ队列及基本操作"><a href="#CQ队列及基本操作" class="headerlink" title="CQ队列及基本操作"></a>CQ队列及基本操作</h2><p>从网络模型图中我们看到每个工作线程都有一个CQ队列，对应的源码如下。</p>
<pre><code>typedef struct conn_queue_item  CQ_ITEM;

struct conn_queue_item {
    int               sfd;
    enum conn_states  init_state;
    int               event_flags;
    int               read_buffer_size;
    enum network_transport     transport;
    CQ_ITEM          *next;
};

/* A connection queue. */
typedef struct conn_queue  CQ;
struct conn_queue {
    CQ_ITEM *head;//指向队列的第一个节点
    CQ_ITEM *tail;//指向队列的最后一个节点
    pthread_mutex_t lock; //一个队列就对应一个锁
};
</code></pre><p>对于每个工作线程来说，这个队列对应的是一个生产者（main thread）和一个消费者（worker thread），但是仍然需要加锁，phread_mutex_t类型的lock就是用来锁住此队列。然后我们来看一下队列的基本操作（push，pop）的源码。</p>
<pre><code>static void cq_init(CQ *cq) {
    pthread_mutex_init(&amp;cq-&gt;lock, NULL);
    cq-&gt;head = NULL;
    cq-&gt;tail = NULL;
}

static CQ_ITEM *cq_pop(CQ *cq) {
    CQ_ITEM *item;

    pthread_mutex_lock(&amp;cq-&gt;lock);
    item = cq-&gt;head;
    if (NULL != item) {
        cq-&gt;head = item-&gt;next;
        if (NULL == cq-&gt;head)
            cq-&gt;tail = NULL;
    }
    pthread_mutex_unlock(&amp;cq-&gt;lock);

    return item;
}

/*
 * Adds an item to a connection queue.
 */
static void cq_push(CQ *cq, CQ_ITEM *item) {
    item-&gt;next = NULL;

    pthread_mutex_lock(&amp;cq-&gt;lock);
    if (NULL == cq-&gt;tail)
          cq-&gt;head = item;
    else
        cq-&gt;tail-&gt;next = item;
    cq-&gt;tail = item;
    pthread_mutex_unlock(&amp;cq-&gt;lock);
}
</code></pre><p>每个worker thread是如何持有这个队列的呢，代码如下。</p>
<pre><code>typedef struct {
    ...
    struct conn_queue *new_conn_queue; /* queue of new connections to handle */
    ...
} LIBEVENT_THREAD;
</code></pre><p>这个LIBEVENT_THREAD顾名思义就是worker线程对应的结构体（为了避免一叶障目，不见森林，我只会在用到时show这部分code，其它部分暂时省略）。</p>
<h2 id="主线程与CQ队列"><a href="#主线程与CQ队列" class="headerlink" title="主线程与CQ队列"></a>主线程与CQ队列</h2><p>当一个连接在主线程建立之后，主线程需要创建一个CQ_ITEM结构体并将它push到对应的工作线程的CQ队列中，这个时候并没有直接去malloc，这样有两个缺点，第一，malloc对应一个系统调用，具有相对昂贵的开销，第二，如果每次需要就malloc容易造成内存碎片，为此，作者使用了内存池，预分配一块比较大的内存，将这块大内存切分成多个CQ_ITEM，代码如下。</p>
<pre><code>//本函数采用了一些优化手段.并非每调用一次本函数就申请一块内存。这会导致
//内存碎片。这里采取的优化方法是，一次性分配64个CQ_ITEM大小的内存(即预分配).
//下次调用本函数的时候，直接从之前分配64个中要一个即可。
//由于是为了防止内存碎片，所以不是以链表的形式放置这64个CQ_ITEM。而是数组的形式。
//于是，cqi_free函数就有点特别了。它并不会真正释放.而是像内存池那样归还
static CQ_ITEM *cqi_new(void) {
    //所有线程都会访问cqi_freelist的。所以需要加锁
    CQ_ITEM *item = NULL;
    pthread_mutex_lock(&amp;cqi_freelist_lock);
    if (cqi_freelist) {
        item = cqi_freelist;
           cqi_freelist = item-&gt;next;
    }
    pthread_mutex_unlock(&amp;cqi_freelist_lock);

    if (NULL == item) {//没有多余的CQ_ITEM了
        int i;

        item = malloc(sizeof(CQ_ITEM) * ITEMS_PER_ALLOC);//该宏等于64

        //item[0]直接返回为调用者，不用next指针连在一起。调用者负责将
        //item[0].next赋值为NULL
        for (i = 2; i &lt; ITEMS_PER_ALLOC; i++)//将这块内存分成一个个的item并且用next指针像链表一样连起来
            item[i - 1].next = &amp;item[i];

            pthread_mutex_lock(&amp;cqi_freelist_lock);
            //因为主线程负责申请CQ_ITEM，子线程负责释放CQ_ITEM。所以cqi_freelist此刻
            //可能并不等于NULL。由于使用头插法，所以无论cqi_freeelist是否为NULL，都能
            //把链表连起来的。
            item[ITEMS_PER_ALLOC - 1].next = cqi_freelist;
            cqi_freelist = &amp;item[1];
            pthread_mutex_unlock(&amp;cqi_freelist_lock);
    }

    return item;
}


//并非释放，而是像内存池那样归还
static void cqi_free(CQ_ITEM *item) {
    pthread_mutex_lock(&amp;cqi_freelist_lock);
    item-&gt;next = cqi_freelist;
    cqi_freelist = item;  //头插法归还
    pthread_mutex_unlock(&amp;cqi_freelist_lock);
}
</code></pre><p>所有工作线程在释放CQ_ITEM的时候会将CQ_ITEM以头插法插入到cqi_freelist，下一次主线程直接取头节点重用，因为存在多个线程去修改它，所以也需要加锁。<br>主线程是如何做到建立连接之后将CQ_ITEM添加到某个工作线程的队列中并通知此线程呢？首先我们来看main函数。</p>
<pre><code>int main (int argc, char **argv) {
    //对memcached的关键设置取默认值
    settings_init();

    //main_base是一个struct event_base类型的全局变量
    main_base = event_init();//为主线程创建一个event_base

    conn_init();//先不管，后面会说到

    //创建settings.num_threads个worker线程，并且为每个worker线程创建一个CQ队列
    //并为这些worker申请各自的event_base，worker线程然后进入事件循环中    
    thread_init(settings.num_threads, main_base);

    //设置一个定时event(也叫超时event)，定时(频率为一秒)更新current_time变量
    //这个超时event是add到全局变量main_base里面的，
    //所以主线程负责更新current_time(这是一个很重要的全局变量)
    clock_handler(0, 0, 0);


    /* create the listening socket, bind it, and init */
    if (settings.socketpath == NULL) {
        FILE *portnumber_file = NULL;
        //创建监听客户端的socket
        //tcp_transport是枚举类型
        if (settings.port &amp;&amp; 
            server_sockets(settings.port, tcp_transport,portnumber_file)) {
            vperror(&quot;failed to listen on TCP port %d&quot;, settings.port);
            exit(EX_OSERR);
        }
    }

    if (event_base_loop(main_base, 0) != 0) {//主线程进入事件循环
        retval = EXIT_FAILURE;
    }

    return retval;
}
</code></pre><p>main函数中创建了属于自己的event_base，thread_init函数是初始化worker thread的，这个我们下面再说，server_sockets负责创建socket并加入可读事件回调，然后主线程就进入event_base_loop事件循环中，下面我们来看看server_sockets函数以及它所调用的其它函数。</p>
<pre><code>//port是默认的11211或者用户使用-p选项设置的端口号
//主线程在main函数会调用本函数
static int server_sockets(int port, 
                          enum network_transport transport,
                          FILE *portnumber_file) {

    //settings.inter里面可能有多个IP地址.如果有多个那么将用逗号分隔
    char *b;
    int ret = 0;
    //复制一个字符串，避免下面的strtok_r函数修改(污染)全局变量settings.inter
    char *list = strdup(settings.inter);

    //这个循环主要是处理多个IP的情况
    for (char *p = strtok_r(list, &quot;;,&quot;, &amp;b);
        p != NULL; //分割出一个个的ip,使用分号;作为分隔符
         p = strtok_r(NULL, &quot;;,&quot;, &amp;b)) {
        int the_port = port;
        char *s = strchr(p, &apos;:&apos;);//启动的可能使用-l ip:port 参数形式
        //ip后面接着端口号，即指定ip的同时也指定了该ip的端口号
        //此时采用ip后面的端口号，而不是采用-p指定的端口号
        if (s != NULL) {
            *s = &apos;\0&apos;;//截断后面的端口号，使得p指向的字符串只是一个ip
            ++s;
            if (!safe_strtol(s, &amp;the_port)) {//非法端口号参数值
                return 1;
            }
        }
        if (strcmp(p, &quot;*&quot;) == 0) {
            p = NULL;
        }
        //处理其中一个IP。有p指定ip(或者hostname)
        ret |= server_socket(p, the_port, transport, portnumber_file);
    }
    free(list);
    return ret;
}


static conn *listen_conn = NULL;//监听队列(可能要同时监听多个IP)

//interface是一个ip、hostname或者NULL。这个ip字符串后面没有端口号。端口号由参数port指出
static int server_socket(const char *interface,
                          int port,
                          enum network_transport transport,
                          FILE *portnumber_file) {
    int sfd;
    struct linger ling = {0, 0};
    struct addrinfo *ai;
    struct addrinfo *next;
    struct addrinfo hints = { .ai_flags = AI_PASSIVE,
                                .ai_family = AF_UNSPEC };
    char port_buf[NI_MAXSERV];
    int success = 0;
    int flags =1;

    hints.ai_socktype = IS_UDP(transport) ? SOCK_DGRAM : SOCK_STREAM;


    snprintf(port_buf, sizeof(port_buf), &quot;%d&quot;, port);
    getaddrinfo(interface, port_buf, &amp;hints, &amp;ai);

    //如果interface是一个hostname的话，那么可能就有多个ip
    for (next= ai; next; next= next-&gt;ai_next) {
        conn *listen_conn_add;

        //创建一个套接字，然后设置为非阻塞的
        sfd = new_socket(next);//调用socket函数
        bind(sfd, next-&gt;ai_addr, next-&gt;ai_addrlen);

        success++;
        listen(sfd, settings.backlog);


        if (!(listen_conn_add = conn_new(sfd, conn_listening,
                                         EV_READ | EV_PERSIST, 1,
                                     t     ransport, main_base))) {
            fprintf(stderr, &quot;failed to create listening connection\n&quot;);
            exit(EXIT_FAILURE);
        }

        //将要监听的多个conn放到一个监听队列里面
        listen_conn_add-&gt;next = listen_conn;
        listen_conn = listen_conn_add;

    }

    freeaddrinfo(ai);

    /* Return zero iff we detected no errors in starting up connections */
    return success == 0;
}


static int new_socket(struct addrinfo *ai) {
    int sfd;
    int flags;
    sfd = socket(ai-&gt;ai_family, ai-&gt;ai_socktype, ai-&gt;ai_protocol);
    flags = fcntl(sfd, F_GETFL, 0);
    fcntl(sfd, F_SETFL, flags | O_NONBLOCK);

    return sfd;
}
</code></pre><p>我们需要关心的是conn_new这个函数，它既会被主线程调用，也会被工作线程调用，下面我们来看看它的源码。</p>
<pre><code>//thread_libevent_process这个管道事件回调使用于主线程接受到客户端连接后通知工作子线程重新创建一个新的
//conn，在conn_new重新设置网络事件回调函数conn_new-&gt;event_handler

//为sfd分配一个conn结构体，并且为这个sfd建立一个event，然后base监听这个event
//这里面会设置网络事件回调函数conn_new-&gt;event_handler
conn *conn_new(const int sfd, enum conn_states init_state,
               const int event_flags, const int read_buffer_size, 
               enum network_transport transport, struct event_base *base) {
    conn *c;

    assert(sfd &gt;= 0 &amp;&amp; sfd &lt; max_fds);
    //直接使用下标
    c = conns[sfd];

    //之前没有哪个连接用过这个sfd值，需要申请一个conn结构体
    if (NULL == c) {
        if (!(c = (conn *)calloc(1, sizeof(conn)))) {
            STATS_LOCK();
            stats.malloc_fails++;
            STATS_UNLOCK();
            fprintf(stderr, &quot;Failed to allocate connection object\n&quot;);
            return NULL;
        }
        MEMCACHED_CONN_CREATE(c);
        //初始化一些成员变量
         c-&gt;sfd = sfd;
        //将这个结构体交由conns数组管理
        conns[sfd] = c;
    }
    ...//初始化另外一些成员变量  
    c-&gt;state = init_state;//值为conn_listening  

    //等同于event_assign，会自动关联current_base。event的回调函数是event_handler  
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);  
    event_base_set(base, &amp;c-&gt;event);  
    c-&gt;ev_flags = event_flags;  

    if (event_add(&amp;c-&gt;event, 0) == -1) {  
        perror(&quot;event_add&quot;);  
        return NULL;  
    }  

    return c;  
}  
</code></pre><p>这个函数最重要的一件事就是为server_socket中建立的socket设置可读事件回调函数event_handler，而且连接的初始状态事conn_listening，这个状态是主线程唯一的状态，下面我们来抽丝剥茧看看event_handler做了什么。</p>
<pre><code>void event_handler(const int fd, const short which, void *arg) {
    conn *c;

    c = (conn *)arg;
    assert(c != NULL);

    c-&gt;which = which;

    /* sanity */
    if (fd != c-&gt;sfd) {
        if (settings.verbose &gt; 0)
            fprintf(stderr, &quot;Catastrophic: event fd doesn&apos;t match conn fd!\n&quot;);
        conn_close(c);
        return;
    }

    drive_machine(c);

    /* wait for next event */
    return;
}
</code></pre><p>它由调用了drive_machine，真是各种调啊，让我们揭开drive_machine的神秘面纱吧。</p>
<pre><code>//主线程接收到客户端连接，通过event_handler触发drive_machine发送信息&quot;c&quot;给工作子线程进行处理
//工作子线程从队列中取出主线程accept的fd,
//thread_libevent_process中取出该CQ_ITEM后，创建新的conn进行事件处理
static void drive_machine(conn *c) {
    bool stop = false;
    int sfd;
    socklen_t addrlen;
    struct sockaddr_storage addr;
    int nreqs = settings.reqs_per_event;//20

    //drive_machine被调用会进行状态判断，并进行一些处理。但也可能发生状态的转换
    //此时需要一个循环，当进行专题转换时，也能处理
    while (!stop) {
        switch(c-&gt;state) {
            case conn_listening: //只有主线程处于该状态
                   addrlen = sizeof(addr);

                sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);                   
                if (settings.maxconns_fast &amp;&amp;
                    stats.curr_conns + stats.reserved_fds &gt;= settings.maxconns - 1) {
                    str = &quot;ERROR Too many open connections\r\n&quot;;
                    res = write(sfd, str, strlen(str));
                    close(sfd);
                    STATS_LOCK();
                    stats.rejected_conns++;
                    STATS_UNLOCK();
                } else {
                    //选定一个worker线程，new一个CQ_ITEM，把这个CQ_ITEM扔给这个线程
                    dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST,
                                 DATA_BUFFER_SIZE, tcp_transport);
                }

            stop = true;
            break;
        }
    }
    return
}
</code></pre><p>drive_machine是一个状态机，包涵了整个网络模型的所有事件状态，这里只是列出了主线程用到的conn_listening状态，dispatch_conn_new函数完成了选定一个worker线程，new一个CQ_ITEM，把这个CQ_ITEM扔给这个线程并通知它，我们来看看它是怎么做到的。</p>
<pre><code>//工作子线程读管道有数据到来，thread_libevent_process从读管道读取到信息
//对应的主线程写管道在dispatch_conn_new或者switch_item_lock_type

//主线程检查到有新的链接到来，则通过管道通知子线程
void dispatch_conn_new(int sfd, enum conn_states init_state, 
                       int event_flags, int read_buffer_size, 
                       enum network_transport transport) {
    CQ_ITEM *item = cqi_new();
    char buf[1];
    //轮询的方式选定一个worker线程
    int tid = (last_thread + 1) % settings.num_threads;

    LIBEVENT_THREAD *thread = threads + tid; //轮询选择由那个工作现场来处理

    last_thread = tid;

    item-&gt;sfd = sfd;
    item-&gt;init_state = init_state;
    item-&gt;event_flags = event_flags;
    item-&gt;read_buffer_size = read_buffer_size;
    item-&gt;transport = transport;
    //把这个item放到选定的worker线程的CQ队列中
    cq_push(thread-&gt;new_conn_queue, item);

    MEMCACHED_CONN_DISPATCH(sfd, thread-&gt;thread_id);
    buf[0] = &apos;c&apos;;
    // 通知worker线程，有新客户单连接到来
    if (write(thread-&gt;notify_send_fd, buf, 1) != 1) {
        perror(&quot;Writing to thread notify pipe&quot;);
    }
}
</code></pre><p>我们可以很清楚的看到，主线程采用轮询的方式选取一个工作线程，调用cq_push将CQ_ITEM放入工作线程的队列中，然后向管道写入一个字符，来唤醒工作线程。<br>至此主线程处理逻辑已经基本完成，由于调用链路较长，我们再重新来简单的梳理下。</p>
<blockquote>
<p>主线程：server_sockets–&gt;server_socket–&gt;conn_new–&gt;event_set(注册socket可读事件回调event_handler)</p>
<p>回调函数：event_handle–&gt;drive_machine–&gt;dispatch_conn_new</p>
</blockquote>
<h2 id="工作线程与CQ队列"><a href="#工作线程与CQ队列" class="headerlink" title="工作线程与CQ队列"></a>工作线程与CQ队列</h2><p>工作线程的初始化在main函数中的thread_init函数中，让我们来看看吧。</p>
<pre><code>//参数nthread是woker线程的数量。main_base则是主线程的event_base
//主线程在main函数调用本函数，创建nthreads个worker线程
void thread_init(int nthreads, struct event_base *main_base) {
    int         i;
    int         power;

    //申请一个CQ_ITEM时需要加锁
    pthread_mutex_init(&amp;cache_lock, NULL);
    pthread_mutex_init(&amp;stats_lock, NULL);

    pthread_mutex_init(&amp;init_lock, NULL);
    pthread_cond_init(&amp;init_cond, NULL);

    pthread_mutex_init(&amp;cqi_freelist_lock, NULL);
    cqi_freelist = NULL;

    item_lock_count = hashsize(power);
    item_lock_hashpower = power;

    //哈希表中段级别的锁。并不是一个桶就对应有一个锁。而是多个桶共用一个锁
    item_locks = calloc(item_lock_count, sizeof(pthread_mutex_t));
    if (! item_locks) {
        perror(&quot;Can&apos;t allocate item locks&quot;);
        exit(1);
    }
    for (i = 0; i &lt; item_lock_count; i++) {
        pthread_mutex_init(&amp;item_locks[i], NULL);
    }
    pthread_key_create(&amp;item_lock_type_key, NULL);
    pthread_mutex_init(&amp;item_global_lock, NULL);

    //申请具有nthreads个元素的LIBEVENT_THREAD数据
    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
    if (! threads) {
        perror(&quot;Can&apos;t allocate thread descriptors&quot;);
        exit(1);
    }
    //主线程对应的
    dispatcher_thread.base = main_base;
    dispatcher_thread.thread_id = pthread_self();

    //子线程的
    for (i = 0; i &lt; nthreads; i++) {
        int fds[2];
        //为每个worker线程分配一个管道，用于通知worker线程
        if (pipe(fds)) {
            perror(&quot;Can&apos;t create notify pipe&quot;);
            exit(1);
        }

        threads[i].notify_receive_fd = fds[0];
        threads[i].notify_send_fd = fds[1];
        //每一个线程配一个event_base，并设置event监听notify_receive_fd的读事件
        //同时还为这个线程分配一个conn_queue队列
        setup_thread(&amp;threads[i]);
        /* Reserve three fds for the libevent base, and two for the pipe */
        stats.reserved_fds += 5;
    }

    /* Create threads after we&apos;ve done all the libevent setup. */
    for (i = 0; i &lt; nthreads; i++) {
         //创建线程，线程函数为worker_libevent，线程参数为&amp;thread[i]
        create_worker(worker_libevent, &amp;threads[i]);
    }

    /* Wait for all the threads to set themselves up before returning. */
    pthread_mutex_lock(&amp;init_lock);
    wait_for_thread_registration(nthreads); //主线程阻塞等待事件到来
    pthread_mutex_unlock(&amp;init_lock);
}
</code></pre><p>代码说明了一切，请注意最后的wait_for_thread_registration函数，主线程将在此阻塞，直到所有的工作线程都初始化完成，我们来看看setup_thread函数。</p>
<pre><code>static void setup_thread(LIBEVENT_THREAD *me) {
    //新建一个event_base
    me-&gt;base = event_init();

    /* Listen for notifications from other threads */
    //监听管道的读端
    event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd,
                EV_READ | EV_PERSIST, thread_libevent_process, me);
    //将event_base和event想关联
    event_base_set(me-&gt;base, &amp;me-&gt;notify_event);

    if (event_add(&amp;me-&gt;notify_event, 0) == -1) {
        fprintf(stderr, &quot;Can&apos;t monitor libevent notify pipe\n&quot;);
        exit(1);
    }
    // 创建一个CQ队列
    me-&gt;new_conn_queue = malloc(sizeof(struct conn_queue));
    if (me-&gt;new_conn_queue == NULL) {
        perror(&quot;Failed to allocate memory for connection queue&quot;);
        exit(EXIT_FAILURE);
    }
    cq_init(me-&gt;new_conn_queue);

    if (pthread_mutex_init(&amp;me-&gt;stats.mutex, NULL) != 0) {
        perror(&quot;Failed to initialize mutex&quot;);
        exit(EXIT_FAILURE);
    }

    me-&gt;suffix_cache = cache_create(&quot;suffix&quot;, SUFFIX_SIZE, sizeof(char*),
                                NULL, NULL);
    if (me-&gt;suffix_cache == NULL) {
        fprintf(stderr, &quot;Failed to create suffix cache\n&quot;);
        exit(EXIT_FAILURE);
    }
}
</code></pre><p>这个函数做的比较重要的事是为管道读端注册可读事件回调thread_libevent_process，用来处理主线程写入的字符，创建了CQ队列，让我们来看看事件回调函数。</p>
<pre><code>//工作子线程读管道有数据到来，thread_libevent_process从读管道读取到信息
//对应的主线程写管道在dispatch_conn_new或者switch_item_lock_type
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    if (read(fd, buf, 1) != 1)
       switch (buf[0]) {
            case &apos;c&apos;: //dispatch_conn_new
            //从CQ队列中读取一个item，因为是pop所以读取后，CQ队列会把这个item从队列中删除
                item = cq_pop(me-&gt;new_conn_queue);

                if (NULL != item) {
            //为sfd分配一个conn结构体，并且为这个sfd建立一个event，然后让base监听这个event
            //这个sfd的事件回调函数是event_handler
                    conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, 
                    item-&gt;event_flags, item-&gt;read_buffer_size, 
                    item-&gt;transport, me-&gt;base);
                } else {
                    c-&gt;thread = me;
                }
                cqi_free(item);
                break;
            //switch_item_lock_type触发走到这里
            /* we were told to flip the lock type and report in */
            case &apos;l&apos;: //参考switch_item_lock_type //切换item到段级别
            //唤醒睡眠在init_cond条件变量上的迁移线程
                me-&gt;item_lock_type = ITEM_LOCK_GRANULAR;
                register_thread_initialized();
                break;
            case &apos;g&apos;://切换item锁到全局级别
                me-&gt;item_lock_type = ITEM_LOCK_GLOBAL;
                register_thread_initialized();
                break;
        }
    }
}
</code></pre><p>我们可以看到工作线程会从自己的队列里面取出主线程push的CQ_ITEM，然后调用conn_new同样注册事件回调event_handler，最后会走到drive_machine，这个逻辑和主线程是一样的。让我们再看看thread_init里面的create_worker函数。</p>
<pre><code>static void create_worker(void *(*func)(void *), void *arg) {
    pthread_t       thread;
    pthread_attr_t  attr;
    int             ret;

    pthread_attr_init(&amp;attr);

    if ((ret = pthread_create(&amp;thread, &amp;attr, func, arg)) != 0) {
        fprintf(stderr, &quot;Can&apos;t create thread: %s\n&quot;,
            strerror(ret));
        exit(1);
    }
}
</code></pre><p>这个函数就是真正的创建工作线程的地方，在所有的回调函数都设置完毕之后就可以启动工作线程了，我们还是来简单的梳理下调用过程。</p>
<blockquote>
<p>主线程：thread_init–&gt;setup_thread(注册可读事件回调thread_libevent_process)</p>
<p>回调函数：thread_libevent_process–&gt;conn_new–&gt;event_handler–&gt;drive_machine</p>
<p>主线程：thread_init–&gt;create_worker–&gt;pthread_create</p>
</blockquote>
<p>这样整个memcached的网络部分就算是过了一遍，如果想详细的了解还得看代码，再次感谢文章开头所参考的博客和代码的作者。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="chenshuiyin" />
          <p class="site-author-name" itemprop="name">chenshuiyin</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenshuiyin</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  
  

  

  

</body>
</html>
